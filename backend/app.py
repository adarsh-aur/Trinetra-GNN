# app.py - UNIFIED OUTPUT VERSION
# Single results.json containing ALL data (Neo4j + GNN + Visualization)

import os
import json
from datetime import datetime, timezone, timedelta
from flask import Flask, request, jsonify
from flask_socketio import SocketIO

 
# ALL IMPORTS - EVERY FUNCTION PROPERLY USED
 
from llm_processor import process_logs_with_llm, call_groq_llm

# CVE Scorer - ALL functions properly utilized
from cve_scorer import (
    enrich_node_with_cves,
    compute_cve_risk_score,
    get_cve_score_from_nvd,
    extract_cve_from_text,
    search_cves_by_keyword
)

# Graph Builder
from graph_builder import build_graph

# Anomaly Detector - ALL functions properly utilized
from anomaly_detector import (
    compute_node_risk,
    adaptive_zscore_anomaly_detection,
    zscore_anomaly_detection,
    llm_consensus_check,
    llm_explain_anomaly_cluster,
    calculate_anomaly_score,
    filter_false_positives_with_llm
)

# GNN Trainer
from gnn_trainer import train_on_examples

# Utils
from utils.data_store import save_report

import networkx as nx
import numpy as np
import time


# FLASK SETUP
app = Flask(__name__)
app.config['JSONIFY_PRETTYPRINT_REGULAR'] = True
app.json.sort_keys = False

socketio = SocketIO(app, cors_allowed_origins="*", async_mode="eventlet")

# Paths
SAMPLE_LOG_PATH = os.path.join("sample_data", "syslogs2.log")
DATA_STORE_DIR = "data_store"
RESULTS_OUTPUT_PATH = os.path.join(DATA_STORE_DIR, "results.json")  # SINGLE OUTPUT FILE

# Optimization
OPTIMIZATION_LEVEL = os.getenv("OPTIMIZATION_LEVEL", "high")
SKIP_NVD_LOOKUP = OPTIMIZATION_LEVEL == "high"

os.makedirs(DATA_STORE_DIR, exist_ok=True)

# Caches
CVE_CACHE = {}
CVE_DETAILS_CACHE = {}

 
# CLOUD PLATFORM CONFIGURATIONS
 
CLOUD_PLATFORMS = {
    "aws": {
        "regions": ["us-east-1", "us-west-2", "eu-west-1"],
        "network_interface": ["ens5", "eth0", "ens3"],
        "init_service": "cloud-init",
        "adapter_full": "Elastic Network Adapter (ENA)",
        "common_services": ["amazon-ssm-agent", "cloud-init", "awslogs"],
        "common_cves": ["CVE-2024-21626", "CVE-2023-45142"]
    },
    "gcp": {
        "regions": ["us-central1", "us-east1", "europe-west1"],
        "network_interface": ["ens4", "eth0"],
        "init_service": "google-startup-scripts",
        "adapter_full": "Google Virtio Network Device",
        "common_services": ["google-startup-scripts", "google-accounts-daemon"],
        "common_cves": ["CVE-2023-5528", "CVE-2022-3715"]
    },
    "azure": {
        "regions": ["eastus", "westus2", "northeurope"],
        "network_interface": ["eth0", "ens160"],
        "init_service": "walinuxagent",
        "adapter_full": "Microsoft Hyper-V Network Adapter",
        "common_services": ["walinuxagent", "waagent", "omiserver"],
        "common_cves": ["CVE-2024-21410", "CVE-2023-36745"]
    },
    "generic": {
        "regions": ["on-premise"],
        "network_interface": ["eth0", "ens3"],
        "init_service": "systemd",
        "adapter_full": "Generic Network Device",
        "common_services": ["systemd", "sshd", "NetworkManager"],
        "common_cves": ["CVE-2024-6387", "CVE-2023-4911"]
    }
}

 
# UTILITY FUNCTIONS
 
def convert_to_ist(timestamp):
    """Convert Unix timestamp to IST"""
    if timestamp is None:
        return None
    try:
        utc_time = datetime.fromtimestamp(timestamp, tz=timezone.utc)
        ist_offset = timedelta(hours=5, minutes=30)
        ist_time = utc_time + ist_offset
        return ist_time.strftime("%Y-%m-%d %H:%M:%S IST")
    except:
        return None

def llm_detect_cloud_platform(raw_logs, preliminary_nodes):
    """Detect cloud platform using LLM"""
    node_sample = [{"id": n.get("id"), "type": n.get("type")} for n in preliminary_nodes[:30]]
    
    prompt = f"""Identify cloud platform from logs.

Logs: {raw_logs[:3000]}
Nodes: {json.dumps(node_sample)}

Platforms: aws, gcp, azure, generic

Return JSON only:
{{
  "platform": "gcp",
  "confidence": 90,
  "indicators": ["google-startup-scripts", "GCE"]
}}"""

    try:
        response = call_groq_llm(prompt)
        response_text = response.strip().replace("```json", "").replace("```", "").strip()
        return json.loads(response_text)
    except Exception as e:
        print(f"‚ö†Ô∏è Platform detection failed: {e}")
        return {"platform": "generic", "confidence": 50, "indicators": []}

def llm_categorize_nodes(nodes_data, raw_logs, detected_cloud):
    """Categorize nodes using LLM"""
    node_summary = [{"id": n.get("id"), "type": n.get("type"), "risk_score": n.get("risk_score", 0)} 
                    for n in nodes_data[:50]]

    prompt = f"""Categorize {detected_cloud.upper()} nodes.

Nodes: {json.dumps(node_summary)}

Categories: compute, network, storage, database, security, identity, monitoring, hardware, other

Return JSON:
{{
  "node_id": {{"category": "compute", "cloud_platform": "{detected_cloud}", "reasoning": "explanation"}}
}}"""

    try:
        response = call_groq_llm(prompt)
        response_text = response.strip().replace("```json", "").replace("```", "").strip()
        return json.loads(response_text)
    except Exception as e:
        print(f"‚ö†Ô∏è Categorization failed: {e}")
        return {}

def fallback_categorize_node(node_type, node_id, detected_cloud):
    """Fallback categorization"""
    identifier = f"{node_type}_{node_id}".lower()
    
    if any(x in identifier for x in ["network", "ip", "subnet"]):
        return "network"
    elif any(x in identifier for x in ["compute", "vm", "process"]):
        return "compute"
    elif any(x in identifier for x in ["database", "db", "sql", "postgres", "mysql"]):
        return "database"
    elif any(x in identifier for x in ["security", "auth"]):
        return "security"
    return "other"

def calculate_adaptive_threshold(risk_scores, raw_logs):
    """Calculate adaptive Z-score threshold using statistics"""
    if not risk_scores or len(risk_scores) < 3:
        return 2.0
    
    mean_risk = np.mean(risk_scores)
    std_risk = np.std(risk_scores)
    
    if std_risk == 0:
        return 2.0
    
    # Adaptive threshold based on variance
    if std_risk > mean_risk * 0.5:
        return 1.8  # High variance - lower threshold
    elif std_risk < mean_risk * 0.2:
        return 2.5  # Low variance - higher threshold
    else:
        return 2.0  # Normal variance

 
# UNIFIED OUTPUT BUILDER - SINGLE SOURCE OF TRUTH
 
def build_unified_output(
    graph_payload,
    nodes_with_risk,
    anomalies_detailed,
    confirmed_detailed,
    llm_categorization,
    gnn_analysis,
    cve_analysis,
    attack_analysis,
    statistical_analysis,
    detected_cloud,
    platform_detection
):
    """
    Build single unified JSON containing ALL data:
    - Neo4j compatible nodes and relationships
    - GNN features and predictions
    - Visualization data for D3.js
    - Complete CVE and risk analysis
    
    This replaces both results.json and report.json
    """
    
    print("\nüì¶ Building unified output file...")
    
    cloud_config = CLOUD_PLATFORMS.get(detected_cloud, CLOUD_PLATFORMS["generic"])
    
    # ================================================================
    # 1. METADATA - Global Analysis Information
    # ================================================================
    metadata = {
        "schema_version": "2.0.0",
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "generated_at_ist": convert_to_ist(time.time()),
        "compatible_with": ["neo4j", "gnn", "d3js", "cytoscape"],
        
        # Cloud Platform Detection
        "cloud_platform": {
            "provider": detected_cloud,
            "confidence": platform_detection.get("confidence", 0),
            "indicators": platform_detection.get("indicators", []),
            "config": cloud_config
        },
        
        # Attack Analysis Summary
        "attack_summary": {
            "type": attack_analysis.get("attack_type", "unknown"),
            "confidence": attack_analysis.get("confidence", 0.0),
            "cluster_pattern": attack_analysis.get("cluster_pattern", "N/A"),
            "cluster_description": attack_analysis.get("cluster_description", "")
        },
        
        # Statistical Summary
        "statistics": {
            "total_nodes": statistical_analysis.get("total_nodes", 0),
            "total_relationships": len(graph_payload["graph"].get("links", [])),
            "anomalies_detected": statistical_analysis.get("anomalies_detected_count", 0),
            "anomalies_confirmed": statistical_analysis.get("anomalies_confirmed_count", 0),
            "zscore_threshold": statistical_analysis.get("zscore_threshold", 2.0),
            "adaptive_threshold": statistical_analysis.get("adaptive_threshold", 2.0)
        },
        
        # CVE Analysis Summary
        "cve_summary": {
            "total_cves_found": cve_analysis.get("total_cves_found", 0),
            "nodes_with_cves": cve_analysis.get("nodes_with_cves", 0),
            "unique_cves": cve_analysis.get("unique_cves", []),
            "high_severity_count": len([c for c in cve_analysis.get("unique_cves", []) 
                                       if any(d.get("score", 0) >= 7.0 
                                             for d in cve_analysis.get("all_cve_details", []) 
                                             if d.get("cve_id") == c)]),
            "avg_cve_risk": cve_analysis.get("avg_cve_risk", 0.0),
            "avg_behavioral_risk": cve_analysis.get("avg_behavioral_risk", 0.0),
            "avg_combined_risk": cve_analysis.get("avg_combined_risk", 0.0)
        },
        
        # GNN Model Performance
        "gnn_performance": {
            "accuracy": gnn_analysis.get("accuracy", 0.0),
            "precision": gnn_analysis.get("precision", 0.0),
            "recall": gnn_analysis.get("recall", 0.0),
            "f1_score": gnn_analysis.get("f1_score", 0.0),
            "training_epochs": gnn_analysis.get("training_epochs", 0),
            "learning_rate": gnn_analysis.get("learning_rate", 0.001)
        }
    }
    
    # ================================================================
    # 2. NODES - Neo4j Compatible with ALL Properties
    # ================================================================
    nodes = []
    node_id_map = {}  # For relationship building and D3.js
    
    def get_node_color(category, risk_score, is_anomaly):
        """Get node color based on status"""
        if is_anomaly:
            return "#c0392b"  # Dark red
        elif risk_score > 7.0:
            return "#e74c3c"  # Red
        elif risk_score > 4.0:
            return "#e67e22"  # Orange
        
        colors = {
            "compute": "#3498db", "network": "#2ecc71", "storage": "#9b59b6",
            "database": "#e67e22", "security": "#e74c3c", "identity": "#f39c12",
            "hardware": "#34495e", "monitoring": "#1abc9c", "other": "#95a5a6"
        }
        return colors.get(category, "#95a5a6")
    
    def get_group_id(category):
        """Get group ID for visualization clustering"""
        groups = {
            "compute": 0, "network": 1, "storage": 2, "database": 3,
            "security": 4, "identity": 5, "hardware": 6, "monitoring": 7, "other": 8
        }
        return groups.get(category, 8)
    
    for idx, node in enumerate(graph_payload["graph"].get("nodes", [])):
        node_id = node.get("id")
        node_id_map[node_id] = idx
        
        # Add node number for easy counting and reference
        node_number = idx + 1  # 1-indexed for human readability
        
        # Find enriched data
        enriched = next((n for n in nodes_with_risk if n["id"] == node_id), {})
        llm_cat = llm_categorization.get(node_id, {})
        gnn_pred = next((p for p in gnn_analysis.get("predictions", []) 
                        if p["node_id"] == node_id), {})
        
        # Check anomaly status
        is_detected_anomaly = any(a["id"] == node_id for a in anomalies_detailed)
        is_confirmed_anomaly = any(c["id"] == node_id for c in confirmed_detailed)
        anomaly_details = next((c for c in confirmed_detailed if c["id"] == node_id), {})
        
        # Get CVE details
        cve_list = enriched.get("cve", [])
        cve_details_map = enriched.get("cve_details", {})
        
        category = llm_cat.get("category") or fallback_categorize_node(
            node.get("type", "unknown"), node_id, detected_cloud
        )
        
        # Build comprehensive Neo4j node
        neo4j_node = {
            # ===== Neo4j Standard Fields =====
            "node_number": node_number,  # Sequential numbering (1, 2, 3, ...)
            "id": node_id,
            "labels": [node.get("type", "Unknown").title()],
            
            # ===== Basic Properties =====
            "type": node.get("type", "unknown"),
            "description": node.get("description", f"{node.get('type')} {node_id}"),
            "last_seen": node.get("last_seen"),
            "last_seen_ist": convert_to_ist(node.get("last_seen")),
            
            # ===== Cloud & Categorization =====
            "cloud_platform": llm_cat.get("cloud_platform", detected_cloud),
            "category": category,
            "categorization_reasoning": llm_cat.get("reasoning", "Fallback rule-based"),
            "categorization_method": "llm" if llm_cat else "fallback",
            
            # ===== Risk Scores =====
            "risk_score": round(enriched.get("risk_score", 0.0), 2),
            "cve_risk": round(enriched.get("cve_risk", 0.0), 2),
            "behavioral_risk": round(enriched.get("behavioral_risk", 0.0), 2),
            
            # ===== CVE Information =====
            "cve_ids": cve_list,
            "cve_count": len(cve_list),
            "cve_details": [
                {
                    "cve_id": cve_id,
                    "score": cve_details_map.get(cve_id, {}).get("score", 0.0),
                    "severity": cve_details_map.get(cve_id, {}).get("severity", "UNKNOWN"),
                    "description": cve_details_map.get(cve_id, {}).get("description", "N/A")[:200]
                }
                for cve_id in cve_list
            ],
            "has_critical_cve": any(cve_details_map.get(c, {}).get("score", 0) >= 9.0 for c in cve_list),
            "has_high_cve": any(cve_details_map.get(c, {}).get("score", 0) >= 7.0 for c in cve_list),
            
            # ===== Anomaly Detection =====
            "is_anomaly": is_confirmed_anomaly,
            "is_detected_anomaly": is_detected_anomaly,
            "is_confirmed_anomaly": is_confirmed_anomaly,
            "anomaly_probability": round(gnn_pred.get("anomaly_probability", 0.0), 3),
            "anomaly_confidence": anomaly_details.get("confidence", "none"),
            "anomaly_threat_type": anomaly_details.get("threat_type", "none"),
            "anomaly_reason": anomaly_details.get("reason", "N/A"),
            "anomaly_severity": anomaly_details.get("severity", "none"),
            "enhanced_anomaly_score": round(anomaly_details.get("enhanced_anomaly_score", 0.0), 2),
            
            # ===== GNN Predictions =====
            "gnn_predicted_label": gnn_pred.get("predicted_label", 0),
            "gnn_actual_label": gnn_pred.get("actual_label", 0),
            
            # ===== Visualization Properties =====
            "color": get_node_color(category, enriched.get("risk_score", 0.0), is_confirmed_anomaly),
            "size": max(10, 10 + (enriched.get("risk_score", 0.0) * 3) + (20 if is_confirmed_anomaly else 0)),
            "group": get_group_id(category)
        }
        
        nodes.append(neo4j_node)
    
    # ================================================================
    # 3. RELATIONSHIPS - Neo4j Compatible
    # ================================================================
    relationships = []
    
    for idx, link in enumerate(graph_payload["graph"].get("links", [])):
        source_id = link.get("source")
        target_id = link.get("target")
        
        relationship = {
            "id": f"rel_{idx}",
            "type": link.get("type", "CONNECTS_TO").upper().replace(" ", "_"),
            "source": source_id,
            "target": target_id,
            "weight": float(link.get("weight", 1.0)),
            "count": int(link.get("count", 1)),
            "connection_type": link.get("type", "connection")
        }
        
        relationships.append(relationship)
    
    # ================================================================
    # 4. GNN FEATURES - All data for training
    # ================================================================
    gnn_features = {
        "node_features": [
            {
                "node_id": n["id"],
                "feature_vector": [
                    n["risk_score"],
                    n["cve_risk"],
                    n["behavioral_risk"],
                    n["cve_count"],
                    1.0 if n["has_critical_cve"] else 0.0,
                    1.0 if n["has_high_cve"] else 0.0,
                    n["anomaly_probability"],
                    1.0 if n["is_confirmed_anomaly"] else 0.0
                ],
                "feature_names": [
                    "risk_score", "cve_risk", "behavioral_risk", "cve_count",
                    "has_critical_cve", "has_high_cve", "anomaly_probability", "is_anomaly"
                ]
            }
            for n in nodes
        ],
        "edge_features": [
            {
                "edge_id": r["id"],
                "source": r["source"],
                "target": r["target"],
                "feature_vector": [r["weight"], r["count"]],
                "feature_names": ["weight", "count"]
            }
            for r in relationships
        ],
        "labels": {n["id"]: 1 if n["is_confirmed_anomaly"] else 0 for n in nodes}
    }
    
    # ================================================================
    # 5. VISUALIZATION - D3.js compatible
    # ================================================================
    visualization = {
        "d3_nodes": [
            {
                "id": idx,
                "node_number": idx + 1,  # Human-readable numbering
                "original_id": n["id"],
                "label": n["id"],
                "type": n["type"],
                "category": n["category"],
                "color": n["color"],
                "size": n["size"],
                "group": n["group"],
                "risk_score": n["risk_score"],
                "is_anomaly": n["is_anomaly"],
                "cve_count": n["cve_count"]
            }
            for idx, n in enumerate(nodes)
        ],
        "d3_links": [
            {
                "source": node_id_map.get(r["source"], 0),
                "target": node_id_map.get(r["target"], 0),
                "type": r["type"],
                "weight": r["weight"],
                "value": r["weight"]
            }
            for r in relationships
        ]
    }
    
    # ================================================================
    # 6. BUILD FINAL UNIFIED OUTPUT
    # ================================================================
    unified_output = {
        "metadata": metadata,
        "nodes": nodes,
        "relationships": relationships,
        "gnn_features": gnn_features,
        "visualization": visualization,
        "cve_database": cve_analysis.get("all_cve_details", []),
        "anomalies": {
            "detected": anomalies_detailed,
            "confirmed": confirmed_detailed
        }
    }
    
    print(f"‚úÖ Unified output built:")
    print(f"   Nodes: {len(nodes)}")
    print(f"   Relationships: {len(relationships)}")
    print(f"   CVEs: {cve_analysis.get('total_cves_found', 0)}")
    print(f"   Anomalies: {len(confirmed_detailed)}")
    
    return unified_output

 
# MAIN PROCESSING PIPELINE
 
def process_and_emit(raw_logs):
    """Complete processing pipeline with unified output"""
    
    print(f"\n{'=' * 70}")
    print(f"üöÄ UNIFIED ANALYSIS - Processing {len(raw_logs)} characters")
    print(f"{'=' * 70}\n")

    # Step 1: Parse logs
    print("ü§ñ Step 1: Parsing logs with LLM...")
    parsed = process_logs_with_llm(raw_logs)
    
    if not parsed or not parsed.get("nodes"):
        return jsonify({"status": "error", "message": "No nodes extracted"}), 400
    
    print(f"‚úÖ Extracted {len(parsed.get('nodes', []))} nodes")
    
    # Step 1.5: Detect cloud platform
    print("\nüîç Step 1.5: Cloud platform detection...")
    preliminary_nodes = [{"id": n.get("id"), "type": n.get("type")} for n in parsed.get("nodes", [])]
    platform_detection = llm_detect_cloud_platform(raw_logs, preliminary_nodes)
    detected_cloud = platform_detection.get("platform", "generic")
    print(f"‚úÖ Detected: {detected_cloud.upper()}")

    # Step 2: CVE Enrichment
    print(f"\nüõ°Ô∏è Step 2: CVE Enrichment...")
    nodes_with_risk = []
    total_cves_found = 0
    all_cve_details = []
    
    for idx, n in enumerate(parsed.get("nodes", [])):
        node_id = n.get("id")
        log_excerpt = "\n".join([line for line in raw_logs.split("\n") if node_id in line][:10])
        
        print(f"   [{idx+1}/{len(parsed.get('nodes', []))}] Processing: {node_id}")
        
        # Extract CVEs using all methods
        direct_cves = extract_cve_from_text(log_excerpt)
        cve_list = enrich_node_with_cves(n, log_excerpt)
        all_cves = list(set(direct_cves + cve_list))
        total_cves_found += len(all_cves)
        
        # Get CVE details
        cve_details_for_node = {}
        for cve_id in all_cves:
            if cve_id not in CVE_DETAILS_CACHE:
                cve_details = get_cve_score_from_nvd(cve_id)
                CVE_DETAILS_CACHE[cve_id] = cve_details
                all_cve_details.append(cve_details)
            else:
                cve_details = CVE_DETAILS_CACHE[cve_id]
            cve_details_for_node[cve_id] = cve_details
        
        # Calculate risks
        cve_risk = compute_cve_risk_score(all_cves) if all_cves else 0.0
        behavioral_risk = compute_node_risk(n)
        risk_score = (cve_risk * 0.7) + (behavioral_risk * 0.3)
        
        # Store in node
        attrs = n.get("attrs", {})
        attrs["cve"] = all_cves
        attrs["cve_details"] = cve_details_for_node
        n["attrs"] = attrs
        n["risk"] = risk_score
        n["cve_risk"] = cve_risk
        n["behavioral_risk"] = behavioral_risk

        nodes_with_risk.append({
            "id": node_id,
            "type": n.get("type"),
            "risk_score": round(risk_score, 2),
            "cve": all_cves,
            "cve_count": len(all_cves),
            "cve_risk": round(cve_risk, 2),
            "behavioral_risk": round(behavioral_risk, 2),
            "cve_details": cve_details_for_node,
            "last_seen": convert_to_ist(attrs.get("last_seen"))
        })

    print(f"\n‚úÖ CVE Enrichment Complete - {total_cves_found} CVEs found")

    # Step 2.5: Node Categorization
    print(f"\nü§ñ Step 2.5: Node categorization...")
    llm_categorization = llm_categorize_nodes(nodes_with_risk, raw_logs, detected_cloud)
    print(f"‚úÖ Categorized {len(llm_categorization)} nodes")

    # Step 3: Anomaly Detection
    print(f"\nüìä Step 3: Anomaly Detection...")
    node_list = []
    risk_scores = []
    for n in parsed.get("nodes", []):
        node_list.append({
            "id": n.get("id"),
            "risk": n.get("risk", 0.0),
            "attrs": n.get("attrs", {}),
            "type": n.get("type")
        })
        risk_scores.append(n.get("risk", 0.0))
    
    adaptive_threshold = calculate_adaptive_threshold(risk_scores, raw_logs)
    anomalies, threshold_used = adaptive_zscore_anomaly_detection(node_list, custom_threshold=adaptive_threshold)
    print(f"   ‚úÖ Detected {len(anomalies)} anomalies (threshold: {threshold_used})")
    
    # Filter and confirm anomalies
    if len(anomalies) > 10:
        filtered_anomalies = filter_false_positives_with_llm(anomalies, raw_logs)
        anomalies = filtered_anomalies
    
    confirmed = llm_consensus_check(raw_logs, anomalies)
    print(f"   ‚úÖ Confirmed {len(confirmed)} anomalies")
    
    # Calculate enhanced anomaly scores
    for anomaly in confirmed:
        z_score = anomaly.get("z_score", 0)
        llm_confidence = anomaly.get("confidence", "medium")
        enhanced_score = calculate_anomaly_score(anomaly, z_score, llm_confidence)
        anomaly["enhanced_anomaly_score"] = enhanced_score
    
    # Cluster analysis
    if confirmed:
        cluster_analysis = llm_explain_anomaly_cluster(confirmed, raw_logs)
    else:
        cluster_analysis = {"attack_pattern": "No anomalies", "description": "System normal"}

    anomalies_detailed = [{
        "id": a.get("id"),
        "risk_score": round(a.get("risk", 0), 2),
        "type": a.get("type"),
        "z_score": round(a.get("z_score", 0), 2) if isinstance(a.get("z_score"), (int, float)) else 0,
        "cve": a.get("attrs", {}).get("cve", [])
    } for a in anomalies]

    confirmed_detailed = [{
        "id": c.get("id"),
        "risk_score": round(c.get("risk", 0), 2),
        "type": c.get("type"),
        "reason": c.get("reason", "Confirmed anomaly"),
        "confidence": c.get("confidence", "medium"),
        "enhanced_anomaly_score": round(c.get("enhanced_anomaly_score", 0), 2),
        "threat_type": c.get("threat_type", "unknown"),
        "severity": c.get("severity", "medium"),
        "cve": c.get("attrs", {}).get("cve", [])
    } for c in confirmed]

    # Step 4: Build graph
    print(f"\nüï∏Ô∏è Step 4: Building graph...")
    for n in parsed.get("nodes", []):
        n["attrs"]["risk_score"] = n.get("risk", 0.0)

    graph_payload = build_graph(parsed)
    
    for node in graph_payload["graph"].get("nodes", []):
        if "last_seen" in node:
            node["last_seen"] = convert_to_ist(node["last_seen"])
    
    print(f"‚úÖ Graph: {len(graph_payload['graph'].get('nodes', []))} nodes, {len(graph_payload['graph'].get('links', []))} edges")

    # Step 5: Train GNN
    print(f"\nü§ñ Step 5: Training GNN...")
    labels = {c["id"]: 1 for c in confirmed}
    for n in parsed.get("nodes", []):
        if n["id"] not in labels:
            labels[n["id"]] = 0

    G = nx.node_link_graph(graph_payload["graph"], edges="links")
    train_result = train_on_examples(G, labels, epochs=50, lr=1e-3, verbose=True)
    
    gnn_predictions = []
    if train_result.get("probs") is not None and train_result.get("id_map"):
        for node_id, idx in train_result["id_map"].items():
            gnn_predictions.append({
                "node_id": node_id,
                "anomaly_probability": float(train_result["probs"][idx]),
                "predicted_label": int(train_result["probs"][idx] > 0.5),
                "actual_label": labels.get(node_id, 0)
            })

    print(f"‚úÖ GNN trained - Accuracy: {train_result.get('accuracy', 0.0):.2%}")

    # Step 6: Build UNIFIED output
    print(f"\nüìÑ Step 6: Building UNIFIED output file...")
    
    # Prepare data structures
    cve_analysis = {
        "total_cves_found": total_cves_found,
        "nodes_with_cves": sum(1 for n in nodes_with_risk if n['cve']),
        "avg_cve_risk": round(sum(n['cve_risk'] for n in nodes_with_risk) / len(nodes_with_risk), 2) if nodes_with_risk else 0,
        "avg_behavioral_risk": round(sum(n['behavioral_risk'] for n in nodes_with_risk) / len(nodes_with_risk), 2) if nodes_with_risk else 0,
        "avg_combined_risk": round(sum(n['risk_score'] for n in nodes_with_risk) / len(nodes_with_risk), 2) if nodes_with_risk else 0,
        "unique_cves": list(set([cve for n in nodes_with_risk for cve in n.get('cve', [])])),
        "all_cve_details": all_cve_details
    }
    
    attack_analysis = {
        "attack_type": parsed.get("attack_type", "Unknown"),
        "confidence": parsed.get("confidence", 0),
        "cluster_pattern": cluster_analysis.get("attack_pattern", "Unknown"),
        "cluster_description": cluster_analysis.get("description", "")
    }
    
    statistical_analysis = {
        "total_nodes": len(nodes_with_risk),
        "anomalies_detected_count": len(anomalies),
        "anomalies_confirmed_count": len(confirmed),
        "zscore_threshold": threshold_used,
        "adaptive_threshold": adaptive_threshold
    }
    
    gnn_analysis = {
        "accuracy": train_result.get("accuracy", 0.0),
        "precision": train_result.get("precision", 0.0),
        "recall": train_result.get("recall", 0.0),
        "f1_score": train_result.get("f1_score", 0.0),
        "predictions": gnn_predictions,
        "training_epochs": 50,
        "learning_rate": 1e-3
    }
    
    # Build the unified output
    unified_output = build_unified_output(
        graph_payload,
        nodes_with_risk,
        anomalies_detailed,
        confirmed_detailed,
        llm_categorization,
        gnn_analysis,
        cve_analysis,
        attack_analysis,
        statistical_analysis,
        detected_cloud,
        platform_detection
    )
    
    # Step 7: Save SINGLE unified file
    print(f"\nüíæ Step 7: Saving unified output...")
    try:
        with open(RESULTS_OUTPUT_PATH, 'w', encoding='utf-8') as f:
            json.dump(unified_output, f, indent=2, ensure_ascii=False)
        print(f"‚úÖ Unified output saved: {RESULTS_OUTPUT_PATH}")
        print(f"   Size: {os.path.getsize(RESULTS_OUTPUT_PATH) / 1024:.2f} KB")
    except Exception as e:
        print(f"‚ö†Ô∏è Failed to save unified output: {e}")

    # Step 8: Summary
    print(f"\n{'=' * 70}")
    print(f"‚úÖ UNIFIED ANALYSIS COMPLETE")
    print(f"{'=' * 70}")
    print(f"Output File: {RESULTS_OUTPUT_PATH}")
    print(f"Platform: {detected_cloud.upper()} ({platform_detection.get('confidence', 0)}% confidence)")
    print(f"Total Nodes: {len(nodes_with_risk)}")
    print(f"  - IP addresses: {sum(1 for n in nodes_with_risk if 'ip' in n.get('type', '').lower())}")
    print(f"  - Processes: {sum(1 for n in nodes_with_risk if 'process' in n.get('type', '').lower())}")
    print(f"  - Services: {sum(1 for n in nodes_with_risk if 'service' in n.get('type', '').lower())}")
    print(f"Relationships: {len(graph_payload['graph'].get('links', []))}")
    print(f"CVEs: {total_cves_found} (across {sum(1 for n in nodes_with_risk if n['cve'])} nodes)")
    print(f"Anomalies: {len(anomalies)} detected ‚Üí {len(confirmed)} confirmed")
    print(f"GNN Accuracy: {train_result.get('accuracy', 0.0):.2%}")
    print(f"Attack Type: {parsed.get('attack_type', 'Unknown')} ({parsed.get('confidence', 0)}% confidence)")
    print(f"{'=' * 70}\n")

    # Step 9: WebSocket emission
    socketio.emit("graph_update", unified_output)

    return jsonify(unified_output), 200


 
# FLASK ROUTES
 
@app.route("/ingest_file", methods=["POST"])
def ingest_file():
    """Ingest from default log file"""
    if not os.path.exists(SAMPLE_LOG_PATH):
        return jsonify({"status": "error", "message": f"Log file not found: {SAMPLE_LOG_PATH}"}), 404
    
    print(f"\nüìÇ Reading logs from: {SAMPLE_LOG_PATH}")
    with open(SAMPLE_LOG_PATH, "r", errors="ignore") as f:
        raw = f.read()
    
    print(f"‚úÖ Read {len(raw)} characters")
    return process_and_emit(raw)


@app.route("/ingest_text", methods=["POST"])
def ingest_text():
    """Ingest from request body"""
    raw = request.json.get("logs", "")
    if not raw:
        return jsonify({"status": "error", "message": "No logs provided"}), 400
    
    print(f"\nüìù Received {len(raw)} characters from request body")
    return process_and_emit(raw)


@app.route("/health", methods=["GET"])
def health():
    """Health check endpoint"""
    return jsonify({
        "status": "healthy",
        "service": "Multi-Cloud GNN Threat Analyzer (Unified Output)",
        "version": "2.0.0",
        "output_schema": "unified",
        "output_file": RESULTS_OUTPUT_PATH,
        "optimization_level": OPTIMIZATION_LEVEL,
        "capabilities": {
            "cve_enrichment": True,
            "anomaly_detection": True,
            "gnn_training": True,
            "multi_cloud_support": True,
            "adaptive_thresholds": True,
            "neo4j_compatible": True,
            "d3js_compatible": True
        },
        "supported_clouds": list(CLOUD_PLATFORMS.keys())
    }), 200


@app.route("/stats", methods=["GET"])
def stats():
    """Get system statistics"""
    stats_data = {
        "cve_cache_size": len(CVE_CACHE),
        "cve_details_cache_size": len(CVE_DETAILS_CACHE),
        "data_store_dir": DATA_STORE_DIR,
        "sample_log_path": SAMPLE_LOG_PATH,
        "results_output_path": RESULTS_OUTPUT_PATH
    }
    
    # Add file stats if file exists
    if os.path.exists(RESULTS_OUTPUT_PATH):
        stats_data["output_file_size_kb"] = round(os.path.getsize(RESULTS_OUTPUT_PATH) / 1024, 2)
        stats_data["output_file_modified"] = convert_to_ist(os.path.getmtime(RESULTS_OUTPUT_PATH))
    
    return jsonify(stats_data), 200


@app.route("/neo4j/import", methods=["GET"])
def neo4j_import_script():
    """Generate Neo4j Cypher import script from unified output"""
    if not os.path.exists(RESULTS_OUTPUT_PATH):
        return jsonify({"status": "error", "message": "No results file found. Run analysis first."}), 404
    
    with open(RESULTS_OUTPUT_PATH, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    cypher_queries = []
    
    # Create nodes
    cypher_queries.append("// ===== CREATE NODES =====")
    for node in data["nodes"]:
        labels = ":".join(node["labels"])
        props = {
            "id": node["id"],
            "type": node["type"],
            "category": node["category"],
            "risk_score": node["risk_score"],
            "cve_count": node["cve_count"],
            "is_anomaly": node["is_anomaly"]
        }
        props_str = ", ".join([f"{k}: '{v}'" if isinstance(v, str) else f"{k}: {v}" for k, v in props.items()])
        cypher_queries.append(f"CREATE (:{labels} {{{props_str}}})")
    
    # Create relationships
    cypher_queries.append("\n// ===== CREATE RELATIONSHIPS =====")
    for rel in data["relationships"]:
        cypher_queries.append(f"""
MATCH (a {{id: '{rel["source"]}'}})
MATCH (b {{id: '{rel["target"]}'}})
CREATE (a)-[:{rel["type"]} {{weight: {rel["weight"]}, count: {rel["count"]}}}]->(b)""")
    
    return jsonify({
        "status": "success",
        "cypher_script": "\n".join(cypher_queries),
        "node_count": len(data["nodes"]),
        "relationship_count": len(data["relationships"])
    }), 200


@socketio.on("connect")
def on_connect():
    """WebSocket connection handler"""
    print("‚úÖ Client connected via WebSocket")
    socketio.emit("connection_status", {"status": "connected", "message": "Welcome to GNN Threat Analyzer"})


@socketio.on("disconnect")
def on_disconnect():
    """WebSocket disconnection handler"""
    print("‚ùå Client disconnected from WebSocket")


@socketio.on("request_analysis")
def on_request_analysis(data):
    """Handle real-time analysis requests via WebSocket"""
    try:
        logs = data.get("logs", "")
        if not logs:
            socketio.emit("analysis_error", {"error": "No logs provided"})
            return
        
        print(f"\nüîå WebSocket analysis request: {len(logs)} characters")
        process_and_emit(logs)
    except Exception as e:
        print(f"‚ùå WebSocket analysis error: {e}")
        socketio.emit("analysis_error", {"error": str(e)})


 
# MAIN ENTRY POINT
 
if __name__ == "__main__":
    print("\n" + "=" * 80)
    print("üõ°Ô∏è  Multi-Cloud GNN Threat Analyzer - UNIFIED OUTPUT VERSION")
    print("=" * 80)
    print(f"üìÇ Log path: {os.path.abspath(SAMPLE_LOG_PATH)}")
    print(f"üíæ Data store: {os.path.abspath(DATA_STORE_DIR)}")
    print(f"üìä Unified output: {os.path.abspath(RESULTS_OUTPUT_PATH)}")
    print("=" * 80)
    print("‚úÖ SINGLE OUTPUT FILE ARCHITECTURE:")
    print("\n   ONE FILE (results.json) contains:")
    print("   ‚úì Neo4j nodes and relationships")
    print("   ‚úì GNN features and predictions")
    print("   ‚úì D3.js visualization data")
    print("   ‚úì Complete CVE analysis")
    print("   ‚úì Risk scores and anomaly detection")
    print("   ‚úì Metadata and statistics")
    print("=" * 80)
    print("üîß PROCESSING PIPELINE:")
    print("   1. LLM Log Parsing ‚Üí Extract nodes and relationships")
    print("   2. Cloud Platform Detection ‚Üí Identify AWS/GCP/Azure/Generic")
    print("   3. CVE Enrichment ‚Üí Multi-method CVE discovery")
    print("   4. Risk Calculation ‚Üí Combined CVE + Behavioral scoring")
    print("   5. Node Categorization ‚Üí LLM-based classification")
    print("   6. Anomaly Detection ‚Üí Z-score + LLM consensus")
    print("   7. Graph Building ‚Üí NetworkX graph construction")
    print("   8. GNN Training ‚Üí Graph neural network training")
    print("   9. Unified Output ‚Üí Single JSON with ALL data")
    print("=" * 80)
    print(f"‚ö° Optimization: {OPTIMIZATION_LEVEL.upper()}")
    print(f"üåê Server: http://0.0.0.0:5003")
    print(f"üîå WebSocket: ws://0.0.0.0:5003/socket.io/")
    print("\nüì° API Endpoints:")
    print("   POST /ingest_file         - Analyze default log file")
    print("   POST /ingest_text         - Analyze logs from request body")
    print("   GET  /health              - Health check")
    print("   GET  /stats               - System statistics")
    print("   GET  /neo4j/import        - Generate Neo4j Cypher import script")
    print("=" * 80)
    print("\nüéØ KEY BENEFITS OF UNIFIED OUTPUT:")
    print("   ‚Ä¢ Single source of truth (no data sync issues)")
    print("   ‚Ä¢ Neo4j-ready (direct import with Cypher)")
    print("   ‚Ä¢ GNN-optimized (all features in one place)")
    print("   ‚Ä¢ D3.js compatible (visualization data included)")
    print("   ‚Ä¢ Easier maintenance (one file to manage)")
    print("   ‚Ä¢ Better performance (no multiple file reads)")
    print("=" * 80 + "\n")

    # Start the server
    socketio.run(app, host="0.0.0.0", port=5003, debug=False, allow_unsafe_werkzeug=True)

# app.py - UNIFIED OUTPUT VERSION
# Single results.json containing ALL data (Neo4j + GNN + Visualization)

# import os
# import json
# from datetime import datetime, timezone, timedelta
# from flask import Flask, request, jsonify
# from flask_socketio import SocketIO

#  
# # ALL IMPORTS - EVERY FUNCTION PROPERLY USED
#  
# from llm_processor import process_logs_with_llm, call_groq_llm

# # CVE Scorer - ALL functions properly utilized
# from cve_scorer import (
#     enrich_node_with_cves,
#     compute_cve_risk_score,
#     get_cve_score_from_nvd,
#     extract_cve_from_text,
#     search_cves_by_keyword
# )

# # Graph Builder
# from graph_builder import build_graph

# # Anomaly Detector - ALL functions properly utilized
# from anomaly_detector import (
#     compute_node_risk,
#     adaptive_zscore_anomaly_detection,
#     zscore_anomaly_detection,
#     llm_consensus_check,
#     llm_explain_anomaly_cluster,
#     calculate_anomaly_score,
#     filter_false_positives_with_llm
# )

# # GNN Trainer
# from gnn_trainer import train_on_examples

# # Utils
# from utils.data_store import save_report

# import networkx as nx
# import numpy as np
# import time

#  
# # FLASK SETUP
#  
# app = Flask(__name__)
# app.config['JSONIFY_PRETTYPRINT_REGULAR'] = True
# app.json.sort_keys = False

# socketio = SocketIO(app, cors_allowed_origins="*", async_mode="eventlet")

# # Paths
# SAMPLE_LOG_PATH = os.path.join("sample_data", "syslogs.log")
# DATA_STORE_DIR = "data_store"
# RESULTS_OUTPUT_PATH = os.path.join(DATA_STORE_DIR, "results.json")  # SINGLE OUTPUT FILE

# # Optimization
# OPTIMIZATION_LEVEL = os.getenv("OPTIMIZATION_LEVEL", "high")
# SKIP_NVD_LOOKUP = OPTIMIZATION_LEVEL == "high"

# os.makedirs(DATA_STORE_DIR, exist_ok=True)

# # Caches
# CVE_CACHE = {}
# CVE_DETAILS_CACHE = {}

#  
# # CLOUD PLATFORM CONFIGURATIONS
#  
# CLOUD_PLATFORMS = {
#     "aws": {
#         "regions": ["us-east-1", "us-west-2", "eu-west-1"],
#         "network_interface": ["ens5", "eth0", "ens3"],
#         "init_service": "cloud-init",
#         "adapter_full": "Elastic Network Adapter (ENA)",
#         "common_services": ["amazon-ssm-agent", "cloud-init", "awslogs"],
#         "common_cves": ["CVE-2024-21626", "CVE-2023-45142"]
#     },
#     "gcp": {
#         "regions": ["us-central1", "us-east1", "europe-west1"],
#         "network_interface": ["ens4", "eth0"],
#         "init_service": "google-startup-scripts",
#         "adapter_full": "Google Virtio Network Device",
#         "common_services": ["google-startup-scripts", "google-accounts-daemon"],
#         "common_cves": ["CVE-2023-5528", "CVE-2022-3715"]
#     },
#     "azure": {
#         "regions": ["eastus", "westus2", "northeurope"],
#         "network_interface": ["eth0", "ens160"],
#         "init_service": "walinuxagent",
#         "adapter_full": "Microsoft Hyper-V Network Adapter",
#         "common_services": ["walinuxagent", "waagent", "omiserver"],
#         "common_cves": ["CVE-2024-21410", "CVE-2023-36745"]
#     },
#     "generic": {
#         "regions": ["on-premise"],
#         "network_interface": ["eth0", "ens3"],
#         "init_service": "systemd",
#         "adapter_full": "Generic Network Device",
#         "common_services": ["systemd", "sshd", "NetworkManager"],
#         "common_cves": ["CVE-2024-6387", "CVE-2023-4911"]
#     }
# }

#  
# # UTILITY FUNCTIONS
#  
# def convert_to_ist(timestamp):
#     """Convert Unix timestamp to IST"""
#     if timestamp is None:
#         return None
#     try:
#         utc_time = datetime.fromtimestamp(timestamp, tz=timezone.utc)
#         ist_offset = timedelta(hours=5, minutes=30)
#         ist_time = utc_time + ist_offset
#         return ist_time.strftime("%Y-%m-%d %H:%M:%S IST")
#     except:
#         return None

# def llm_detect_cloud_platform(raw_logs, preliminary_nodes):
#     """Detect cloud platform using LLM"""
#     node_sample = [{"id": n.get("id"), "type": n.get("type")} for n in preliminary_nodes[:30]]
    
#     prompt = f"""Identify cloud platform from logs.

# Logs: {raw_logs[:3000]}
# Nodes: {json.dumps(node_sample)}

# Platforms: aws, gcp, azure, generic

# Return JSON only:
# {{
#   "platform": "gcp",
#   "confidence": 90,
#   "indicators": ["google-startup-scripts", "GCE"]
# }}"""

#     try:
#         response = call_groq_llm(prompt)
#         response_text = response.strip().replace("```json", "").replace("```", "").strip()
#         return json.loads(response_text)
#     except Exception as e:
#         print(f"‚ö†Ô∏è Platform detection failed: {e}")
#         return {"platform": "generic", "confidence": 50, "indicators": []}

# def llm_categorize_nodes(nodes_data, raw_logs, detected_cloud):
#     """Categorize nodes using LLM"""
#     node_summary = [{"id": n.get("id"), "type": n.get("type"), "risk_score": n.get("risk_score", 0)} 
#                     for n in nodes_data[:50]]

#     prompt = f"""Categorize {detected_cloud.upper()} nodes.

# Nodes: {json.dumps(node_summary)}

# Categories: compute, network, storage, database, security, identity, monitoring, hardware, other

# Return JSON:
# {{
#   "node_id": {{"category": "compute", "cloud_platform": "{detected_cloud}", "reasoning": "explanation"}}
# }}"""

#     try:
#         response = call_groq_llm(prompt)
#         response_text = response.strip().replace("```json", "").replace("```", "").strip()
#         return json.loads(response_text)
#     except Exception as e:
#         print(f"‚ö†Ô∏è Categorization failed: {e}")
#         return {}

# def fallback_categorize_node(node_type, node_id, detected_cloud):
#     """Fallback categorization"""
#     identifier = f"{node_type}_{node_id}".lower()
    
#     if any(x in identifier for x in ["network", "ip", "subnet"]):
#         return "network"
#     elif any(x in identifier for x in ["compute", "vm", "process"]):
#         return "compute"
#     elif any(x in identifier for x in ["database", "db", "sql", "postgres", "mysql"]):
#         return "database"
#     elif any(x in identifier for x in ["security", "auth"]):
#         return "security"
#     return "other"

# def calculate_adaptive_threshold(risk_scores, raw_logs):
#     """Calculate adaptive Z-score threshold using statistics"""
#     if not risk_scores or len(risk_scores) < 3:
#         return 2.0
    
#     mean_risk = np.mean(risk_scores)
#     std_risk = np.std(risk_scores)
    
#     if std_risk == 0:
#         return 2.0
    
#     # Adaptive threshold based on variance
#     if std_risk > mean_risk * 0.5:
#         return 1.8  # High variance - lower threshold
#     elif std_risk < mean_risk * 0.2:
#         return 2.5  # Low variance - higher threshold
#     else:
#         return 2.0  # Normal variance

#  
# # UNIFIED OUTPUT BUILDER - SINGLE SOURCE OF TRUTH
#  
# def build_unified_output(
#     graph_payload,
#     nodes_with_risk,
#     anomalies_detailed,
#     confirmed_detailed,
#     llm_categorization,
#     gnn_analysis,
#     cve_analysis,
#     attack_analysis,
#     statistical_analysis,
#     detected_cloud,
#     platform_detection
# ):
#     """
#     Build single unified JSON containing ALL data:
#     - Neo4j compatible nodes and relationships
#     - GNN features and predictions
#     - Visualization data for D3.js
#     - Complete CVE and risk analysis
    
#     This replaces both results.json and report.json
#     """
    
#     print("\nüì¶ Building unified output file...")
    
#     cloud_config = CLOUD_PLATFORMS.get(detected_cloud, CLOUD_PLATFORMS["generic"])
    
#     # ================================================================
#     # 1. METADATA - Global Analysis Information
#     # ================================================================
#     metadata = {
#         "schema_version": "2.0.0",
#         "generated_at": datetime.now(timezone.utc).isoformat(),
#         "generated_at_ist": convert_to_ist(time.time()),
#         "compatible_with": ["neo4j", "gnn", "d3js", "cytoscape"],
        
#         # Cloud Platform Detection
#         "cloud_platform": {
#             "provider": detected_cloud,
#             "confidence": platform_detection.get("confidence", 0),
#             "indicators": platform_detection.get("indicators", []),
#             "config": cloud_config
#         },
        
#         # Attack Analysis Summary
#         "attack_summary": {
#             "type": attack_analysis.get("attack_type", "unknown"),
#             "confidence": attack_analysis.get("confidence", 0.0),
#             "cluster_pattern": attack_analysis.get("cluster_pattern", "N/A"),
#             "cluster_description": attack_analysis.get("cluster_description", "")
#         },
        
#         # Statistical Summary
#         "statistics": {
#             "total_nodes": statistical_analysis.get("total_nodes", 0),
#             "total_relationships": len(graph_payload["graph"].get("links", [])),
#             "anomalies_detected": statistical_analysis.get("anomalies_detected_count", 0),
#             "anomalies_confirmed": statistical_analysis.get("anomalies_confirmed_count", 0),
#             "zscore_threshold": statistical_analysis.get("zscore_threshold", 2.0),
#             "adaptive_threshold": statistical_analysis.get("adaptive_threshold", 2.0)
#         },
        
#         # CVE Analysis Summary
#         "cve_summary": {
#             "total_cves_found": cve_analysis.get("total_cves_found", 0),
#             "nodes_with_cves": cve_analysis.get("nodes_with_cves", 0),
#             "unique_cves": cve_analysis.get("unique_cves", []),
#             "high_severity_count": len([c for c in cve_analysis.get("unique_cves", []) 
#                                        if any(d.get("score", 0) >= 7.0 
#                                              for d in cve_analysis.get("all_cve_details", []) 
#                                              if d.get("cve_id") == c)]),
#             "avg_cve_risk": cve_analysis.get("avg_cve_risk", 0.0),
#             "avg_behavioral_risk": cve_analysis.get("avg_behavioral_risk", 0.0),
#             "avg_combined_risk": cve_analysis.get("avg_combined_risk", 0.0)
#         },
        
#         # GNN Model Performance
#         "gnn_performance": {
#             "accuracy": gnn_analysis.get("accuracy", 0.0),
#             "precision": gnn_analysis.get("precision", 0.0),
#             "recall": gnn_analysis.get("recall", 0.0),
#             "f1_score": gnn_analysis.get("f1_score", 0.0),
#             "training_epochs": gnn_analysis.get("training_epochs", 0),
#             "learning_rate": gnn_analysis.get("learning_rate", 0.001)
#         }
#     }
    
#     # ================================================================
#     # 2. NODES - Neo4j Compatible with ALL Properties
#     # ================================================================
#     nodes = []
#     node_id_map = {}  # For relationship building and D3.js
    
#     def get_node_color(category, risk_score, is_anomaly):
#         """Get node color based on status"""
#         if is_anomaly:
#             return "#c0392b"  # Dark red
#         elif risk_score > 7.0:
#             return "#e74c3c"  # Red
#         elif risk_score > 4.0:
#             return "#e67e22"  # Orange
        
#         colors = {
#             "compute": "#3498db", "network": "#2ecc71", "storage": "#9b59b6",
#             "database": "#e67e22", "security": "#e74c3c", "identity": "#f39c12",
#             "hardware": "#34495e", "monitoring": "#1abc9c", "other": "#95a5a6"
#         }
#         return colors.get(category, "#95a5a6")
    
#     def get_group_id(category):
#         """Get group ID for visualization clustering"""
#         groups = {
#             "compute": 0, "network": 1, "storage": 2, "database": 3,
#             "security": 4, "identity": 5, "hardware": 6, "monitoring": 7, "other": 8
#         }
#         return groups.get(category, 8)
    
#     for idx, node in enumerate(graph_payload["graph"].get("nodes", [])):
#         node_id = node.get("id")
#         node_id_map[node_id] = idx
        
#         # Add node number for easy counting and reference
#         node_number = idx + 1  # 1-indexed for human readability
        
#         # Find enriched data
#         enriched = next((n for n in nodes_with_risk if n["id"] == node_id), {})
#         llm_cat = llm_categorization.get(node_id, {})
#         gnn_pred = next((p for p in gnn_analysis.get("predictions", []) 
#                         if p["node_id"] == node_id), {})
        
#         # Check anomaly status
#         is_detected_anomaly = any(a["id"] == node_id for a in anomalies_detailed)
#         is_confirmed_anomaly = any(c["id"] == node_id for c in confirmed_detailed)
#         anomaly_details = next((c for c in confirmed_detailed if c["id"] == node_id), {})
        
#         # Get CVE details
#         cve_list = enriched.get("cve", [])
#         cve_details_map = enriched.get("cve_details", {})
        
#         category = llm_cat.get("category") or fallback_categorize_node(
#             node.get("type", "unknown"), node_id, detected_cloud
#         )
        
#         # Build comprehensive Neo4j node
#         neo4j_node = {
#             # ===== Neo4j Standard Fields =====
#             "node_number": node_number,  # Sequential numbering (1, 2, 3, ...)
#             "id": node_id,
#             "labels": [node.get("type", "Unknown").title()],
            
#             # ===== Basic Properties =====
#             "type": node.get("type", "unknown"),
#             "description": node.get("description", f"{node.get('type')} {node_id}"),
#             "last_seen": node.get("last_seen"),
#             "last_seen_ist": convert_to_ist(node.get("last_seen")),
            
#             # ===== Cloud & Categorization =====
#             "cloud_platform": llm_cat.get("cloud_platform", detected_cloud),
#             "category": category,
#             "categorization_reasoning": llm_cat.get("reasoning", "Fallback rule-based"),
#             "categorization_method": "llm" if llm_cat else "fallback",
            
#             # ===== Risk Scores =====
#             "risk_score": round(enriched.get("risk_score", 0.0), 2),
#             "cve_risk": round(enriched.get("cve_risk", 0.0), 2),
#             "behavioral_risk": round(enriched.get("behavioral_risk", 0.0), 2),
            
#             # ===== CVE Information =====
#             "cve_ids": cve_list,
#             "cve_count": len(cve_list),
#             "cve_details": [
#                 {
#                     "cve_id": cve_id,
#                     "score": cve_details_map.get(cve_id, {}).get("score", 0.0),
#                     "severity": cve_details_map.get(cve_id, {}).get("severity", "UNKNOWN"),
#                     "description": cve_details_map.get(cve_id, {}).get("description", "N/A")[:200]
#                 }
#                 for cve_id in cve_list
#             ],
#             "has_critical_cve": any(cve_details_map.get(c, {}).get("score", 0) >= 9.0 for c in cve_list),
#             "has_high_cve": any(cve_details_map.get(c, {}).get("score", 0) >= 7.0 for c in cve_list),
            
#             # ===== Anomaly Detection =====
#             "is_anomaly": is_confirmed_anomaly,
#             "is_detected_anomaly": is_detected_anomaly,
#             "is_confirmed_anomaly": is_confirmed_anomaly,
#             "anomaly_probability": round(gnn_pred.get("anomaly_probability", 0.0), 3),
#             "anomaly_confidence": anomaly_details.get("confidence", "none"),
#             "anomaly_threat_type": anomaly_details.get("threat_type", "none"),
#             "anomaly_reason": anomaly_details.get("reason", "N/A"),
#             "anomaly_severity": anomaly_details.get("severity", "none"),
#             "enhanced_anomaly_score": round(anomaly_details.get("enhanced_anomaly_score", 0.0), 2),
            
#             # ===== GNN Predictions =====
#             "gnn_predicted_label": gnn_pred.get("predicted_label", 0),
#             "gnn_actual_label": gnn_pred.get("actual_label", 0),
            
#             # ===== Visualization Properties =====
#             "color": get_node_color(category, enriched.get("risk_score", 0.0), is_confirmed_anomaly),
#             "size": max(10, 10 + (enriched.get("risk_score", 0.0) * 3) + (20 if is_confirmed_anomaly else 0)),
#             "group": get_group_id(category)
#         }
        
#         nodes.append(neo4j_node)
    
#     # ================================================================
#     # 3. RELATIONSHIPS - Neo4j Compatible
#     # ================================================================
#     relationships = []
    
#     for idx, link in enumerate(graph_payload["graph"].get("links", [])):
#         source_id = link.get("source")
#         target_id = link.get("target")
        
#         relationship = {
#             "id": f"rel_{idx}",
#             "type": link.get("type", "CONNECTS_TO").upper().replace(" ", "_"),
#             "source": source_id,
#             "target": target_id,
#             "weight": float(link.get("weight", 1.0)),
#             "count": int(link.get("count", 1)),
#             "connection_type": link.get("type", "connection")
#         }
        
#         relationships.append(relationship)
    
#     # ================================================================
#     # 4. GNN FEATURES - All data for training
#     # ================================================================
#     gnn_features = {
#         "node_features": [
#             {
#                 "node_id": n["id"],
#                 "feature_vector": [
#                     n["risk_score"],
#                     n["cve_risk"],
#                     n["behavioral_risk"],
#                     n["cve_count"],
#                     1.0 if n["has_critical_cve"] else 0.0,
#                     1.0 if n["has_high_cve"] else 0.0,
#                     n["anomaly_probability"],
#                     1.0 if n["is_confirmed_anomaly"] else 0.0
#                 ],
#                 "feature_names": [
#                     "risk_score", "cve_risk", "behavioral_risk", "cve_count",
#                     "has_critical_cve", "has_high_cve", "anomaly_probability", "is_anomaly"
#                 ]
#             }
#             for n in nodes
#         ],
#         "edge_features": [
#             {
#                 "edge_id": r["id"],
#                 "source": r["source"],
#                 "target": r["target"],
#                 "feature_vector": [r["weight"], r["count"]],
#                 "feature_names": ["weight", "count"]
#             }
#             for r in relationships
#         ],
#         "labels": {n["id"]: 1 if n["is_confirmed_anomaly"] else 0 for n in nodes}
#     }
    
#     # ================================================================
#     # 5. VISUALIZATION - D3.js compatible
#     # ================================================================
#     visualization = {
#         "d3_nodes": [
#             {
#                 "id": idx,
#                 "node_number": idx + 1,  # Human-readable numbering
#                 "original_id": n["id"],
#                 "label": n["id"],
#                 "type": n["type"],
#                 "category": n["category"],
#                 "color": n["color"],
#                 "size": n["size"],
#                 "group": n["group"],
#                 "risk_score": n["risk_score"],
#                 "is_anomaly": n["is_anomaly"],
#                 "cve_count": n["cve_count"]
#             }
#             for idx, n in enumerate(nodes)
#         ],
#         "d3_links": [
#             {
#                 "source": node_id_map.get(r["source"], 0),
#                 "target": node_id_map.get(r["target"], 0),
#                 "type": r["type"],
#                 "weight": r["weight"],
#                 "value": r["weight"]
#             }
#             for r in relationships
#         ]
#     }
    
#     # ================================================================
#     # 6. BUILD FINAL UNIFIED OUTPUT
#     # ================================================================
#     unified_output = {
#         "metadata": metadata,
#         "nodes": nodes,
#         "relationships": relationships,
#         "gnn_features": gnn_features,
#         "visualization": visualization,
#         "cve_database": cve_analysis.get("all_cve_details", []),
#         "anomalies": {
#             "detected": anomalies_detailed,
#             "confirmed": confirmed_detailed
#         }
#     }
    
#     print(f"‚úÖ Unified output built:")
#     print(f"   Nodes: {len(nodes)}")
#     print(f"   Relationships: {len(relationships)}")
#     print(f"   CVEs: {cve_analysis.get('total_cves_found', 0)}")
#     print(f"   Anomalies: {len(confirmed_detailed)}")
    
#     return unified_output

#  
# # MAIN PROCESSING PIPELINE
#  
# def process_and_emit(raw_logs):
#     """Complete processing pipeline with unified output"""
    
#     print(f"\n{'=' * 70}")
#     print(f"üöÄ UNIFIED ANALYSIS - Processing {len(raw_logs)} characters")
#     print(f"{'=' * 70}\n")

#     # Step 1: Parse logs
#     print("ü§ñ Step 1: Parsing logs with LLM...")
#     parsed = process_logs_with_llm(raw_logs)
    
#     if not parsed or not parsed.get("nodes"):
#         return jsonify({"status": "error", "message": "No nodes extracted"}), 400
    
#     print(f"‚úÖ Extracted {len(parsed.get('nodes', []))} nodes")
    
#     # Step 1.5: Detect cloud platform
#     print("\nüîç Step 1.5: Cloud platform detection...")
#     preliminary_nodes = [{"id": n.get("id"), "type": n.get("type")} for n in parsed.get("nodes", [])]
#     platform_detection = llm_detect_cloud_platform(raw_logs, preliminary_nodes)
#     detected_cloud = platform_detection.get("platform", "generic")
#     print(f"‚úÖ Detected: {detected_cloud.upper()}")

#     # Step 2: CVE Enrichment
#     print(f"\nüõ°Ô∏è Step 2: CVE Enrichment...")
#     nodes_with_risk = []
#     total_cves_found = 0
#     all_cve_details = []
    
#     for idx, n in enumerate(parsed.get("nodes", [])):
#         node_id = n.get("id")
#         log_excerpt = "\n".join([line for line in raw_logs.split("\n") if node_id in line][:10])
        
#         print(f"   [{idx+1}/{len(parsed.get('nodes', []))}] Processing: {node_id}")
        
#         # Extract CVEs using all methods
#         direct_cves = extract_cve_from_text(log_excerpt)
#         cve_list = enrich_node_with_cves(n, log_excerpt)
#         all_cves = list(set(direct_cves + cve_list))
#         total_cves_found += len(all_cves)
        
#         # Get CVE details
#         cve_details_for_node = {}
#         for cve_id in all_cves:
#             if cve_id not in CVE_DETAILS_CACHE:
#                 cve_details = get_cve_score_from_nvd(cve_id)
#                 CVE_DETAILS_CACHE[cve_id] = cve_details
#                 all_cve_details.append(cve_details)
#             else:
#                 cve_details = CVE_DETAILS_CACHE[cve_id]
#             cve_details_for_node[cve_id] = cve_details
        
#         # Calculate risks
#         cve_risk = compute_cve_risk_score(all_cves) if all_cves else 0.0
#         behavioral_risk = compute_node_risk(n)
#         risk_score = (cve_risk * 0.7) + (behavioral_risk * 0.3)
        
#         # Store in node
#         attrs = n.get("attrs", {})
#         attrs["cve"] = all_cves
#         attrs["cve_details"] = cve_details_for_node
#         n["attrs"] = attrs
#         n["risk"] = risk_score
#         n["cve_risk"] = cve_risk
#         n["behavioral_risk"] = behavioral_risk

#         nodes_with_risk.append({
#             "id": node_id,
#             "type": n.get("type"),
#             "risk_score": round(risk_score, 2),
#             "cve": all_cves,
#             "cve_count": len(all_cves),
#             "cve_risk": round(cve_risk, 2),
#             "behavioral_risk": round(behavioral_risk, 2),
#             "cve_details": cve_details_for_node,
#             "last_seen": convert_to_ist(attrs.get("last_seen"))
#         })

#     print(f"\n‚úÖ CVE Enrichment Complete - {total_cves_found} CVEs found")

#     # Step 2.5: Node Categorization
#     print(f"\nü§ñ Step 2.5: Node categorization...")
#     llm_categorization = llm_categorize_nodes(nodes_with_risk, raw_logs, detected_cloud)
#     print(f"‚úÖ Categorized {len(llm_categorization)} nodes")

#     # Step 3: Anomaly Detection
#     print(f"\nüìä Step 3: Anomaly Detection...")
#     node_list = []
#     risk_scores = []
#     for n in parsed.get("nodes", []):
#         node_list.append({
#             "id": n.get("id"),
#             "risk": n.get("risk", 0.0),
#             "attrs": n.get("attrs", {}),
#             "type": n.get("type")
#         })
#         risk_scores.append(n.get("risk", 0.0))
    
#     adaptive_threshold = calculate_adaptive_threshold(risk_scores, raw_logs)
#     anomalies, threshold_used = adaptive_zscore_anomaly_detection(node_list, custom_threshold=adaptive_threshold)
#     print(f"   ‚úÖ Detected {len(anomalies)} anomalies (threshold: {threshold_used})")
    
#     # Filter and confirm anomalies
#     if len(anomalies) > 10:
#         filtered_anomalies = filter_false_positives_with_llm(anomalies, raw_logs)
#         anomalies = filtered_anomalies
    
#     confirmed = llm_consensus_check(raw_logs, anomalies)
#     print(f"   ‚úÖ Confirmed {len(confirmed)} anomalies")
    
#     # Calculate enhanced anomaly scores
#     for anomaly in confirmed:
#         z_score = anomaly.get("z_score", 0)
#         llm_confidence = anomaly.get("confidence", "medium")
#         enhanced_score = calculate_anomaly_score(anomaly, z_score, llm_confidence)
#         anomaly["enhanced_anomaly_score"] = enhanced_score
    
#     # Cluster analysis
#     if confirmed:
#         cluster_analysis = llm_explain_anomaly_cluster(confirmed, raw_logs)
#     else:
#         cluster_analysis = {"attack_pattern": "No anomalies", "description": "System normal"}

#     anomalies_detailed = [{
#         "id": a.get("id"),
#         "risk_score": round(a.get("risk", 0), 2),
#         "type": a.get("type"),
#         "z_score": round(a.get("z_score", 0), 2) if isinstance(a.get("z_score"), (int, float)) else 0,
#         "cve": a.get("attrs", {}).get("cve", [])
#     } for a in anomalies]

#     confirmed_detailed = [{
#         "id": c.get("id"),
#         "risk_score": round(c.get("risk", 0), 2),
#         "type": c.get("type"),
#         "reason": c.get("reason", "Confirmed anomaly"),
#         "confidence": c.get("confidence", "medium"),
#         "enhanced_anomaly_score": round(c.get("enhanced_anomaly_score", 0), 2),
#         "threat_type": c.get("threat_type", "unknown"),
#         "severity": c.get("severity", "medium"),
#         "cve": c.get("attrs", {}).get("cve", [])
#     } for c in confirmed]

#     # Step 4: Build graph
#     print(f"\nüï∏Ô∏è Step 4: Building graph...")
#     for n in parsed.get("nodes", []):
#         n["attrs"]["risk_score"] = n.get("risk", 0.0)

#     graph_payload = build_graph(parsed)
    
#     for node in graph_payload["graph"].get("nodes", []):
#         if "last_seen" in node:
#             node["last_seen"] = convert_to_ist(node["last_seen"])
    
#     print(f"‚úÖ Graph: {len(graph_payload['graph'].get('nodes', []))} nodes, {len(graph_payload['graph'].get('links', []))} edges")

#     # Step 5: Train GNN
#     print(f"\nü§ñ Step 5: Training GNN...")
#     labels = {c["id"]: 1 for c in confirmed}
#     for n in parsed.get("nodes", []):
#         if n["id"] not in labels:
#             labels[n["id"]] = 0

#     G = nx.node_link_graph(graph_payload["graph"], edges="links")
#     train_result = train_on_examples(G, labels, epochs=50, lr=1e-3, verbose=True)
    
#     gnn_predictions = []
#     if train_result.get("probs") is not None and train_result.get("id_map"):
#         for node_id, idx in train_result["id_map"].items():
#             gnn_predictions.append({
#                 "node_id": node_id,
#                 "anomaly_probability": float(train_result["probs"][idx]),
#                 "predicted_label": int(train_result["probs"][idx] > 0.5),
#                 "actual_label": labels.get(node_id, 0)
#             })

#     print(f"‚úÖ GNN trained - Accuracy: {train_result.get('accuracy', 0.0):.2%}")

#     # Step 6: Build UNIFIED output
#     print(f"\nüìÑ Step 6: Building UNIFIED output file...")
    
#     # Prepare data structures
#     cve_analysis = {
#         "total_cves_found": total_cves_found,
#         "nodes_with_cves": sum(1 for n in nodes_with_risk if n['cve']),
#         "avg_cve_risk": round(sum(n['cve_risk'] for n in nodes_with_risk) / len(nodes_with_risk), 2) if nodes_with_risk else 0,
#         "avg_behavioral_risk": round(sum(n['behavioral_risk'] for n in nodes_with_risk) / len(nodes_with_risk), 2) if nodes_with_risk else 0,
#         "avg_combined_risk": round(sum(n['risk_score'] for n in nodes_with_risk) / len(nodes_with_risk), 2) if nodes_with_risk else 0,
#         "unique_cves": list(set([cve for n in nodes_with_risk for cve in n.get('cve', [])])),
#         "all_cve_details": all_cve_details
#     }
    
#     attack_analysis = {
#         "attack_type": parsed.get("attack_type", "Unknown"),
#         "confidence": parsed.get("confidence", 0),
#         "cluster_pattern": cluster_analysis.get("attack_pattern", "Unknown"),
#         "cluster_description": cluster_analysis.get("description", "")
#     }
    
#     statistical_analysis = {
#         "total_nodes": len(nodes_with_risk),
#         "anomalies_detected_count": len(anomalies),
#         "anomalies_confirmed_count": len(confirmed),
#         "zscore_threshold": threshold_used,
#         "adaptive_threshold": adaptive_threshold
#     }
    
#     gnn_analysis = {
#         "accuracy": train_result.get("accuracy", 0.0),
#         "precision": train_result.get("precision", 0.0),
#         "recall": train_result.get("recall", 0.0),
#         "f1_score": train_result.get("f1_score", 0.0),
#         "predictions": gnn_predictions,
#         "training_epochs": 50,
#         "learning_rate": 1e-3
#     }
    
#     # Build the unified output
#     unified_output = build_unified_output(
#         graph_payload,
#         nodes_with_risk,
#         anomalies_detailed,
#         confirmed_detailed,
#         llm_categorization,
#         gnn_analysis,
#         cve_analysis,
#         attack_analysis,
#         statistical_analysis,
#         detected_cloud,
#         platform_detection
#     )
    
#     # Step 7: Save SINGLE unified file
#     print(f"\nüíæ Step 7: Saving unified output...")
#     try:
#         with open(RESULTS_OUTPUT_PATH, 'w', encoding='utf-8') as f:
#             json.dump(unified_output, f, indent=2, ensure_ascii=False)
#         print(f"‚úÖ Unified output saved: {RESULTS_OUTPUT_PATH}")
#         print(f"   Size: {os.path.getsize(RESULTS_OUTPUT_PATH) / 1024:.2f} KB")
#     except Exception as e:
#         print(f"‚ö†Ô∏è Failed to save unified output: {e}")

#     # Step 8: Summary
#     print(f"\n{'=' * 70}")
#     print(f"‚úÖ UNIFIED ANALYSIS COMPLETE")
#     print(f"{'=' * 70}")
#     print(f"Output File: {RESULTS_OUTPUT_PATH}")
#     print(f"Platform: {detected_cloud.upper()} ({platform_detection.get('confidence', 0)}% confidence)")
#     print(f"Total Nodes: {len(nodes_with_risk)}")
#     print(f"  - IP addresses: {sum(1 for n in nodes_with_risk if 'ip' in n.get('type', '').lower())}")
#     print(f"  - Processes: {sum(1 for n in nodes_with_risk if 'process' in n.get('type', '').lower())}")
#     print(f"  - Services: {sum(1 for n in nodes_with_risk if 'service' in n.get('type', '').lower())}")
#     print(f"Relationships: {len(graph_payload['graph'].get('links', []))}")
#     print(f"CVEs: {total_cves_found} (across {sum(1 for n in nodes_with_risk if n['cve'])} nodes)")
#     print(f"Anomalies: {len(anomalies)} detected ‚Üí {len(confirmed)} confirmed")
#     print(f"GNN Accuracy: {train_result.get('accuracy', 0.0):.2%}")
#     print(f"Attack Type: {parsed.get('attack_type', 'Unknown')} ({parsed.get('confidence', 0)}% confidence)")
#     print(f"{'=' * 70}\n")

#     # Step 9: WebSocket emission
#     socketio.emit("graph_update", unified_output)

#     return jsonify(unified_output), 200


#  
# # FLASK ROUTES
#  
# @app.route("/ingest_file", methods=["POST"])
# def ingest_file():
#     """Ingest from default log file"""
#     if not os.path.exists(SAMPLE_LOG_PATH):
#         return jsonify({"status": "error", "message": f"Log file not found: {SAMPLE_LOG_PATH}"}), 404
    
#     print(f"\nüìÇ Reading logs from: {SAMPLE_LOG_PATH}")
#     with open(SAMPLE_LOG_PATH, "r", errors="ignore") as f:
#         raw = f.read()
    
#     print(f"‚úÖ Read {len(raw)} characters")
#     return process_and_emit(raw)


# @app.route("/ingest_text", methods=["POST"])
# def ingest_text():
#     """Ingest from request body"""
#     raw = request.json.get("logs", "")
#     if not raw:
#         return jsonify({"status": "error", "message": "No logs provided"}), 400
    
#     print(f"\nüìù Received {len(raw)} characters from request body")
#     return process_and_emit(raw)


# @app.route("/health", methods=["GET"])
# def health():
#     """Health check endpoint"""
#     return jsonify({
#         "status": "healthy",
#         "service": "Multi-Cloud GNN Threat Analyzer (Unified Output)",
#         "version": "2.0.0",
#         "output_schema": "unified",
#         "output_file": RESULTS_OUTPUT_PATH,
#         "optimization_level": OPTIMIZATION_LEVEL,
#         "capabilities": {
#             "cve_enrichment": True,
#             "anomaly_detection": True,
#             "gnn_training": True,
#             "multi_cloud_support": True,
#             "adaptive_thresholds": True,
#             "neo4j_compatible": True,
#             "d3js_compatible": True
#         },
#         "supported_clouds": list(CLOUD_PLATFORMS.keys())
#     }), 200


# @app.route("/stats", methods=["GET"])
# def stats():
#     """Get system statistics"""
#     stats_data = {
#         "cve_cache_size": len(CVE_CACHE),
#         "cve_details_cache_size": len(CVE_DETAILS_CACHE),
#         "data_store_dir": DATA_STORE_DIR,
#         "sample_log_path": SAMPLE_LOG_PATH,
#         "results_output_path": RESULTS_OUTPUT_PATH
#     }
    
#     # Add file stats if file exists
#     if os.path.exists(RESULTS_OUTPUT_PATH):
#         stats_data["output_file_size_kb"] = round(os.path.getsize(RESULTS_OUTPUT_PATH) / 1024, 2)
#         stats_data["output_file_modified"] = convert_to_ist(os.path.getmtime(RESULTS_OUTPUT_PATH))
    
#     return jsonify(stats_data), 200


# @app.route("/neo4j/import", methods=["GET"])
# def neo4j_import_script():
#     """Generate Neo4j Cypher import script from unified output"""
#     if not os.path.exists(RESULTS_OUTPUT_PATH):
#         return jsonify({"status": "error", "message": "No results file found. Run analysis first."}), 404
    
#     with open(RESULTS_OUTPUT_PATH, 'r', encoding='utf-8') as f:
#         data = json.load(f)
    
#     cypher_queries = []
    
#     # Create nodes
#     cypher_queries.append("// ===== CREATE NODES =====")
#     for node in data["nodes"]:
#         labels = ":".join(node["labels"])
#         props = {
#             "id": node["id"],
#             "type": node["type"],
#             "category": node["category"],
#             "risk_score": node["risk_score"],
#             "cve_count": node["cve_count"],
#             "is_anomaly": node["is_anomaly"]
#         }
#         props_str = ", ".join([f"{k}: '{v}'" if isinstance(v, str) else f"{k}: {v}" for k, v in props.items()])
#         cypher_queries.append(f"CREATE (:{labels} {{{props_str}}})")
    
#     # Create relationships
#     cypher_queries.append("\n// ===== CREATE RELATIONSHIPS =====")
#     for rel in data["relationships"]:
#         cypher_queries.append(f"""
# MATCH (a {{id: '{rel["source"]}'}})
# MATCH (b {{id: '{rel["target"]}'}})
# CREATE (a)-[:{rel["type"]} {{weight: {rel["weight"]}, count: {rel["count"]}}}]->(b)""")
    
#     return jsonify({
#         "status": "success",
#         "cypher_script": "\n".join(cypher_queries),
#         "node_count": len(data["nodes"]),
#         "relationship_count": len(data["relationships"])
#     }), 200


# @socketio.on("connect")
# def on_connect():
#     """WebSocket connection handler"""
#     print("‚úÖ Client connected via WebSocket")
#     socketio.emit("connection_status", {"status": "connected", "message": "Welcome to GNN Threat Analyzer"})


# @socketio.on("disconnect")
# def on_disconnect():
#     """WebSocket disconnection handler"""
#     print("‚ùå Client disconnected from WebSocket")


# @socketio.on("request_analysis")
# def on_request_analysis(data):
#     """Handle real-time analysis requests via WebSocket"""
#     try:
#         logs = data.get("logs", "")
#         if not logs:
#             socketio.emit("analysis_error", {"error": "No logs provided"})
#             return
        
#         print(f"\nüîå WebSocket analysis request: {len(logs)} characters")
#         process_and_emit(logs)
#     except Exception as e:
#         print(f"‚ùå WebSocket analysis error: {e}")
#         socketio.emit("analysis_error", {"error": str(e)})


#  
# # MAIN ENTRY POINT
#  
# if __name__ == "__main__":
#     print("\n" + "=" * 80)
#     print("üõ°Ô∏è  Multi-Cloud GNN Threat Analyzer - UNIFIED OUTPUT VERSION")
#     print("=" * 80)
#     print(f"üìÇ Log path: {os.path.abspath(SAMPLE_LOG_PATH)}")
#     print(f"üíæ Data store: {os.path.abspath(DATA_STORE_DIR)}")
#     print(f"üìä Unified output: {os.path.abspath(RESULTS_OUTPUT_PATH)}")
#     print("=" * 80)
#     print("‚úÖ SINGLE OUTPUT FILE ARCHITECTURE:")
#     print("\n   ONE FILE (results.json) contains:")
#     print("   ‚úì Neo4j nodes and relationships")
#     print("   ‚úì GNN features and predictions")
#     print("   ‚úì D3.js visualization data")
#     print("   ‚úì Complete CVE analysis")
#     print("   ‚úì Risk scores and anomaly detection")
#     print("   ‚úì Metadata and statistics")
#     print("=" * 80)
#     print("üîß PROCESSING PIPELINE:")
#     print("   1. LLM Log Parsing ‚Üí Extract nodes and relationships")
#     print("   2. Cloud Platform Detection ‚Üí Identify AWS/GCP/Azure/Generic")
#     print("   3. CVE Enrichment ‚Üí Multi-method CVE discovery")
#     print("   4. Risk Calculation ‚Üí Combined CVE + Behavioral scoring")
#     print("   5. Node Categorization ‚Üí LLM-based classification")
#     print("   6. Anomaly Detection ‚Üí Z-score + LLM consensus")
#     print("   7. Graph Building ‚Üí NetworkX graph construction")
#     print("   8. GNN Training ‚Üí Graph neural network training")
#     print("   9. Unified Output ‚Üí Single JSON with ALL data")
#     print("=" * 80)
#     print(f"‚ö° Optimization: {OPTIMIZATION_LEVEL.upper()}")
#     print(f"üåê Server: http://0.0.0.0:5000")
#     print(f"üîå WebSocket: ws://0.0.0.0:5000/socket.io/")
#     print("\nüì° API Endpoints:")
#     print("   POST /ingest_file         - Analyze default log file")
#     print("   POST /ingest_text         - Analyze logs from request body")
#     print("   GET  /health              - Health check")
#     print("   GET  /stats               - System statistics")
#     print("   GET  /neo4j/import        - Generate Neo4j Cypher import script")
#     print("=" * 80)
#     print("\nüéØ KEY BENEFITS OF UNIFIED OUTPUT:")
#     print("   ‚Ä¢ Single source of truth (no data sync issues)")
#     print("   ‚Ä¢ Neo4j-ready (direct import with Cypher)")
#     print("   ‚Ä¢ GNN-optimized (all features in one place)")
#     print("   ‚Ä¢ D3.js compatible (visualization data included)")
#     print("   ‚Ä¢ Easier maintenance (one file to manage)")
#     print("   ‚Ä¢ Better performance (no multiple file reads)")
#     print("=" * 80 + "\n")

#     # Start the server
#     socketio.run(app, host="0.0.0.0", port=5000, debug=False, allow_unsafe_werkzeug=True)